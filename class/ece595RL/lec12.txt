Good. Okay, so first of all, how is everyone?
What's the fundamental difference between reinforcement learning and supervised learning? Supervised learning you have training test and then you are looking at the labels, let's say data and that's how you are treating the data. But in reinforcement learning, you take a whole different approach where you're trying to. Well, maize optimized, yeah. Rewards and returns. Optimizinforceptimiz'timzation. Don't optimize your rewards. It's not maximization because maximization means optimization. Good, nice. We are ready for the me term. I feel safe.

Now, what is the fundamental characteristic of reinforcement learning? What you get a reward based on the actions that you take. How you take your actions allowed, not necessary policy. You may just have probabilities. Okay? Not necessarily a policy based on the probabilities. After you take an action, you receive a reward.
What is the goal of reinforcement learning? Who talks, what return, how we call that return, and how we call that long term, long term return. Expected return.
Why we call it expected return? Because we have the, because we have the, you said that earlier, that's why I'm asking you again. The probabilities. Because we have the probabilities. Okay. So it's not a maximization, it's practically maximization of the expected return because it is totally probabilistic. 
I can repeat The question was what is the fundamental difference between RL and supervised learning? In the supervised learning, what we said, it's what he said. He said very nicely. We have a set of data, The data are labeled, and we try to perform classification on the data in a fundamental way. Okay? We are not doing supervised learning here. While the main characteristic of the RL is that you have a set of actions, a set of corresponding rewards that, that you are receiving. By taking the actions, you select the actions how probabilistically and the ultimate goal is to maximize your expected return. Now if we discuss specifically for the MDBs on top of the actions, rewards and your expected return, you have the concept of the. If we are discussing about the MD guys, I am in the third coffee of the day. I need to slow down. If we are I record? Yes. Okay. If we are discussing about the MDBs. The MDBs are also characterized by the state. We have the actions, the states, the rewards, the probabilities with the probability consist of the dynamic function. And there you said that earlier. 
Good. How does reinforcement learning differ from unsupervised learning in terms of the learning process? Hands go ahead. Unsupervised. Well, for unsupervised learning, you try to, it's more of a categorization. You try to do some clustering to make meaning from the data that you have. With reinforcement learning, you, when you're maximizing your expected returns, those can change. So it's moving all over the place. While unsupervised learning is fixed, let me keep up on what you are saying. The questions are for the whole audience. That's the unsupervised learning. In the unsupervised learning to interact with the environment. In the unsupervised learning, we have a clear goal. No, while at the RL, we have a clear goal and we take our actions, yes, probabilistically. And what is the goal that we just discussed? To maximize the expected return. Okay. In the unsupervised learning, do you have a clear objective of what you're trying to achieve? Which practically in the RL. Okay, let me reask the unsupervised learning. Do you have a clear objective like what you're trying to achieve? No. You just try to classify. Remember the example with the cats and the dogs And I saw that baby Bella. I mean, we identified the set of the dogs and I said, okay, now I give you a photo of baby Bella, and you understand that baby Bella, it's a dog. Okay. So in the unsupervised learning, you do not have a clear objective. While in L, you do have a clear objective, which your clear objective is there of continue maximizing the expecting, expected return. Don't say just optimization in the exam because I will have a question about if the RL is optimization. Okay, And apparently the answer is no. Okay, cool. You don't optimize your policy. You optimize your policy in the MDP when you're solving the Bellman optimality equation. It's not in general in the RL clear. Good.
Okay, what role does agent play in reinforcement learning scenario? What is the agent, what is or what she does? Tell me that. Bravo. She is a decision maker in order to make decisions because it's she, she explores how she does the exploration by interacting with the environment. Something that does not happen in supervised or unsupervised learning. What else she has? She has a clear on Ben, she has a clear the agent in L has a clear what is the main challenge in RL to balance exploration and exploitation.
Would you define an environment in the context of L? Sorry? How would you define an environment in the context of L? How you identify your environment? Remember, we had two or three slides saying where the environment stops, the agent starts. It's what the agent can't directly control with its actions. Cannot, the environment has states. What did I say? You cannot control it. You may. Let me remind an example that I had said for James. When you load your data to server one, I have 10 gigabytes in my pocket. I offload 6 gigabytes. That's the action that I take. I may know that if I offload a lot of data during the afternoon, let's say 12:00 P.M. that everyone is working. Or 11:00 A.M. Usually at 12:00 P.M. people get a lunch break at 11:00 A.M. The server may be congested. I may have a priori knowledge that my latency will be worse. This is part of the environment. I don't really control the environment, but I may have knowledge about how the environment will, how I will interact with the environment. Meaning what rewards. I'm going to experience clear. Do you want me to repeat? I may have knowledge about how the environment, how I will interact with the environment and how my actions will impact the outcome that stems from the hosticty of the environment. It doesn't mean that the outcome becomes deterministic, but I may have a knowledge of how the outcome can be determined, but as you said, I don't have control over that. Two points about the environment. The difference between the boundary, the agent, the environment stop. Wherever the agent does not have control over the environment, those are the boundaries. But the agent may have knowledge about how it interacts with the environment and how the rewards are calculated in the environment. Do I need to slow down? Okay, good. Let's continue. 

Can you provide an example of an action and the reward in a simple reinforcement learning task? you talked if I'm trying to find the fastest route home, my action would be to take a left hand turn as opposed to a right hand turn. And my time would either go up or down, based on how good that decision was. So what is your reward? Getting home? Faster, shorter distance, how you call it, travel time. The trouble time. Okay, nice. Does anyone else want to give another example? I'm scared, you're laughing. I take it. If I want to see the board better, I can take the action to jump into that chair. And my reward is how good I can see the board. We all understood that he tries to pretend he's a good student. Any other example, a little bit more engineering rather than jumping on on, on chairs. Thank you, Timmy. Kay. Be wrong here. But if you have like three different servers and you want to offload your data on to the server, depending on which server you offload on, If you're going to server to you and offload all your data there, your reward is how much data you can offload to that server. The amount of how I freak out the student's Your word, my kid. So you're offloading the data to your action? Yeah, you reword the amount of Stop repeating. I'm trying to think. I can think of it. All of a sudden that's a reward latency. Good. Any other example that it's a little bit more creative rather than what I'm saying in class?
Imagine I have a certain amount of targets and they want to choose base stations. They could choose these base stations and their utility may increase. Any other example that it's not part of your Phd give it to be you're trying to use the fastest for transform algorithm. The information you're given is the size of the problem. And your reward is based off of how fast the particular algorithm you chose runs, who still does 15 nowadays. Nice example. That was just a bad joke.

what is the purpose of a policy in reinforcement learning? Okay, nice. The last one was nice. Let me repeat for our line students, the policy practically it's a probability vector that demonstrates, that captures the probability of selecting an action when you are in a state. No. Okay, what is important for a learning agent?
Why is it important for a learning agent to explore different actions?  Exploring different actions may result in a higher long term reward versus the alternative, which would be, I guess, trying to exploit your short term rewards that may result in less rather than you explore. Excellent, and let me continue to this question why it's not the best policy, It's time to select the maximum of the potential rewards that you can receive based on the state that you are essentially asking about. Like if we take the greedy approach all the time, we short term, we might be able to get some high rewards. But versus exploration, the just, for example, if we have a path finding situation by exploring, we might be able to find the, have a shorter path by exploring versus exploiting. If someone wants to be smart, which I'm not going to be in the exam. I'm just saying if anyone questions you, is it always deterministic that by doing exploration, you can always receive higher expected return? At a specific time horizon? No. Why? Now that you know the MDB is why? No, no, no, no, no. I want something simpler, he said because it's computationally feasible. What was my question? Yes. Why? I cannot claim that by doing exploration, I will always achieve a better expected return in a specific time horizon if someone wants to challenge me. Why the greed? It's not always. I mean, it doesn't mean that by doing exploration, I will always achieve better than the grid. Why is that? I was just going to say you might just agreed he might be the right one, you might get higher because yes, to say that a little bit more in the MDB concept is because you may have two policies, that they are both optimal. The grid can be equivalent uo another policy with another corresponding probabilistic vector, which at the end of the day, it gives you the same expected return. It's not for sure that you will always do better. That's a little bit tricky question, but I'm just saying if anyone out there challenges you, okay, let me continue.

How does the exploration and exploitation of impact the learning process? I return your question back to you when you asked me what is better, how to do it, how much exploration, how much exploitation to Francisco. The exploration makes the learning slower, but it allows you to reach a higher convergence point. When you say higher convergence point, I will rephrase your answer, and I say a potentially higher expected reward. I agree. With exploration, you sacrifice computational, computational complexity in terms of doing always the greedy, which do always the greedy is the exploitation. On the other hand, if you weigh more your exploitation, repeat what you said, the exploration, if you do more exploitation, what is the danger that you have? That you'll be able to reap the short term rewards but not reach your ultimate reward that you would get otherwise from exploring. Repeat questions. Okay, sure, let me remember the question. The question was, how does exploration exploitation trade of impact the learning process? If you do more exploration, you have higher probability of achieving, in the long term, a higher expected return with the cost of her computational complexity. Because you explore the exploitation, if you weigh more your exploitation, you have the danger of being trapped in a lower expected return at the end of the day. But computationally, it's less complex. Okay, I move.
Can you give an example of a situation where an agent needs to balance exploration and exploitation? I mean, real life example again. Yeah, let me repeat. Sorry guys. I'll try to slow down. Can you give an example of a situation where an agent needs to balance exploration and exploitation? I think that your degree at the end of the day will write Masters or potentially PD. You need to optimize your long term expect return in computer engineering. Just saying you're trying to find the fastest FFT algorithm, You're characterizing it based off of the program that's asking for the result and the size of the input data. You might have a general FFT algorithm that runs usually pretty fast, but may not be the fastest for your specific process than other example a bit away from how you got stuck with FFT. Anyone else from back there that I hope that you, all of you. One example could be like navigation. You can have a robot that has a sensor that needs to navigate to a goal and has obstacles randomly distributed in the environment. It'll need to do some exploration to around, to get a mapping of the environment, but then it'll have to do some exploitation to find the shortest path to its goal. Nice. You have an example, talk to the NBA. If you want to saturate a market and offload a bunch of stocks on a bunch of people, you want to make sure that you easing, you're selling the stocks at the highest rate that you can sell it without causing the other trading algorithms to notice that you're off loading a bunch of stock. You need to check prices as they go up and down and factor in that to how much you actually sell as a poor, excellent, You've lived many years in California. If I ask, let's say at the exam, give me a health care related example, or give me a smart grid related example. Are you capable of giving an example where an agent needs to balance exploration and exploitation? Okay. Can somebody give me an example like that? Going away from the service, the FFTs, the navigation? Please save me. Go ahead. I believe in you with health care, you need to know how to treat people, but you won't ever know what medicine is unless you just give people a bunch of the ice. Please give me a lot of weird things that's exploration and then the exploitation where you're going to perform it today. You will never treat people if you don't treat people, okay, nice example. Thank you. Thanks for using the poor mice. Any other example from smart grid? Gian Carlo in the balance field. In the case of exploration, you're exploring possible causes of potential cancers. And exploitation is based off of certain information that you already know that that is very likely to cause cancer. And so you might approach a certain treatment based on that. Nice example. A thank you. Give me an example from a smart grid. Let's say that you do it on purpose. I believe it. Smart grid. Let's say that in your house you have photovoltaics. And I'm going to say the example, I'm not working over there. Let's say you have in your house, photovolticssumer, you generate energy yourself to see what is the optimal price to sell it to the market. You are going to explore different price levels to see what is the actions of the other neighboring prosumers to buy energy from you. And you also will perform exploitation by selling at the price that will maximize your profit. That's the greedy strategy, okay? Do I move?
Good, nice. What does it mean for a policy to be optimal in reinforcement learning? James? Very nice. You missed a word. He said. Okay, let me ask again the question and try to think what word he missed. I asked, what does it mean for a policy to be optimal in reinforcement learning? And he said policy, it's optimal when it chooses at its stage, the action that maximizes, you said, the expected return. Bra's, the expected expected return, because the first expected comes from the expectation and the second expected comes from there. If you remember the formula. It's expectation of the GT given that you are in a state and you're taking an action, okay? If you talk about the action value function, if you talk for the state value function, it's the expected expected return when you are in a state, okay? What expected return? One of the expected expectation of the other one is from the discount factor times the other ones from the GT, which the GD includes the discount factor that you said expect exactly good.
How does the concept of cumulative rewards relates to the long term goals of an agent? Let me repeat. Can you repeat? Yes, I can repeat slower. How does the concept of cumulative rewards relate to the long term goals of an agent? I say I cannot understand the question by reading so slowly. Okay, How does the concept rewards relate to the long term goals of an agent? Think how we write the GD function. You use your cumulative rewards to calculate your expected return, your GT in the future. They are multiplied by the gain, the discount factor, your current reward. Yes, I know there's like a difference between episodic and continuous, but the books Okay, what's the question if we answered that GT is equal to the sum of all rewards in an episodic one. In a continuous one, it's like gamma times RT gamma squared and so on. That would that be correct? Come on kid. I mean, in the episodic, it's just the summation will be bounded. In the continuous, it's the summation will go to the infinity. It's the same concept. It's the same concept. Okay, good. 
Okay, can you provide examples of industries where reinforcement learning has some promising results? Smart grid. You're a smart kid. A bit of justification. Let's start throwing names of companies industries smart grid, and you can give one sentence sentence, answer Smart grid, The agent can be, for example, the prosumer selling energy, which the amount of selling energy is the action to her prosumers. And the reward can be the price, the profit that it makes, smart grid. When you said IOT, what do you have in mind? Iot, like the social internet of things, you can have a communication network versus IOT devices that need to communicate with each other. You can use RL to find how best they can communicate with each other to let's say offload data at your wife says action. So the actions would be the devices offloading the data that they need to, the expected return can be either like latency or if they're able to offload the data. And then, well, you might have stuff like interference, right? Because you might have to agents taking the action to offload all their data. And you can have interference, right? That, that might be one metric, right? But you can exchange the metrics matrices, okay? You love the microphone. Other example, James. I'll try, He said the social Internet of things. You have devices that they are participate in a social network and they float data like let's say that I want to send you over messenger. I am an IOT device with my smartphone and I want to send you over the messenger photo, baby Bella. Okay, I flow data to my action is the data flow can be, the reward can be, for example, you make a lie on the photo. Or the reward can be the inverse of the latency that I experienced to load the data to you. Or the reward can be the inverse of the packet error rate, et cetera. Any other example? Again, you stay away from the targets. I saw an example of a guy who wrote a reinforcement learning algorithm to play a video game online and defeat. Everybody knows on Youtube is amazing. Good, nice. You may make wait wait action action. Actually, it was really complex. There's a lot of actions like team selection and then move selection, and then the reward was winning at the end. It didn't have the chess example you gave. It wasn't just as simple things as the end. That's a nice example. Anything else? Don't repeat the previous thing. Car manufacturers want to develop how to make their cars parallel park themselves. They have an end goal and they have actions about how much to turn the wheel, how much to press the gas, when to reverse. The reward is getting closer and closer and being in the parallel in the parking spot. Okay. So to the chest example, you get you get one? Yes. Can you repeat it? Yeah. The reward you get nothing. If you don't end up parked or you like hit a car, you get one reward. If you park without hitting anything, don't hit the card.
Think simply, what is the central conflict that the multi armed bandit problem addresses? It's a little bit tricky. Remember the multi banded problem. It's a typical arre algorithm, simple just with actions. You just pull those arms with a probability and you receive a reward. You louder. I don't understand. What do you mean neither? What do you mean? But the environment, the whole banded problem itself. Like the probabilities of, I don't know, like bringing all the fruits together. I don't know how you say that. I'm not sure that I understand. What do you mean? Yeah, dynamic. It's really random. It's totally stochastic. That's what I mean. Okay, let me ask again the question. I'm sorry. It's a little bit tricky. So I will rephrase. The question is, what is the central conflict that the multi banded problem addresses? Let me generalize a little bit. What is the most important problem that the algorithms who say aloud, what do you mean? Balancing exploration and exploitation. In the exam, I ask, what is the central conflict that the multi arm bandit problem addresses, Ben? But if tomorrow, tomorrow at October 18. I ask you in the exam, what is the central conflict that the UCB algorithm addresses? Come on guys. All those are algorithm just changed the name. What's again, the main challenge that it has? The exam, I ask you, what is the main challenge that the grad algorithm mad, If in the future you learn more algorithms, and in the final exam I ask you, what is the main challenge that the Q learning algorithm madres? Now you remember a question for the final. It has, yeah. You can blame start. The questions are from them, Canadians, not even Greeks. Okay. I think this question we already answered it, but I'm going to ask it. It's just in the concept of multi banded.
The question says, how does exploration and exploitation trade off play a critical role in solving the multi bonded problems? We said that earlier. Can somebody repeat? Okay, let me ask again. How does the exploration exploitation tradeoff play a critical role in solving the multi bonded problem? The exploration helps you find which arms may have long term rewards, always being greedy early on, in comparison to always being greedy early on. The drawback of exploration, it's more complex. Yeah, it takes more computation to decide how to explore and the respect to explode. It takes more time to converge on an optimal solution on the other exploitation. The exploitation, its purpose is to maximize your return from the slaw machines, get to get the most money from the gambling machines. That's what it's benefit is. You converge early on a policy, you converge early on, you converge on a policy, What is your final expect, okay? Drawback is back exploitation. The exploitation is that you're kind of stuck on the same bandit machine. So you get stuck practically in a solution that may not really be the optimal. Okay, do I move the exploration? There's also a problem which you get lost. Like you can be over exploring and getting lost in the sauce. Never really finding a good ones that like an American phrase. Yeah, nice. Now I'm going to start using it. Just need to find the right context. Okay, any question I move. Are we still together?
Okay, What do the arms in the multi arm band problem symbolize? Who said that aloud? Different actions. The answer to what do the arms in the multi band problem symbolizes? The arms. Are you using P over the Bravo for not using it? Brava James? No, that's a stupid question. Sorry, I think we asked that like if in the exam I asked you why it's necessary for an agent to explore new actions to potentially get a higher long term reward. And if I ask that, sorry, I know that you may think that I'm stupid by asking again the same and the same questions. But I hope that if I ask the same questions in the final, you're going to answer them. Experience says that half of the class doesn't write it excessive
How can excessive exploration exploitation impact the agents learning process? Too much. Excessive with doubles, I think. Yes. Okay, Let's ask the first question, how excessive exploration you can answer to me either. You need to answer both positively and negatively, How excessive exploration can impact the learning process of the agent. You'll find a solution with the higher expected return. That's the p, that's the positive. And the negative is you might get lost in the sauce. Second time today. Second time today.
How excessive exploitation impacts the learning process. Guys, the others that are quiet are you just feel sorry for me for running with the high heels all around or you just have no idea what we've discussed in this class the last one month, 1.5 You feel sorry for me? It's good to feel sorry for me. I feel sorry for me as well. Exploitation, excessive exploitation. The downside is that it can take a long time to converge. Excessive exploitation. Yeah, If you only explore a little bit, right? You say exploitation of exploration, exploitation can take a long time to converge because it takes a long time to find the optimal actions. Excessive, too much exploitation, you're going to take longer to an optimal solution. Yes answer to that, no, because you may get stuck to a local optimal, you're going to convert faster because you don't explore space. Because you don't explore your space though, you won't convert the solution that you're going to find won't be optimal. Let me rephrase that a bit. It may not be as good as if you were performing as exploration. Again, remember, there can't be a smart guy that comes and tells you, what does it mean if you do exploration? Is it deterministic that you always will do better than only doing exploitation? The answer is no. The reason is that you may have to, when you say solutions, if we talk in the words of MDP to optimal policy. Okay, let's be a little bit careful with the wording. Yes, go ahead. The conflict in that conversation was that was because I, it's that when you excessively exploit, you're not going to explore the space enough to find a potential, potential better policy. Exactly. Okay. I even gave the answer in Greek God. Is it clear? Do I move? Okay, nice.
Can you provide examples of industries or situations where the multi arm bandit problem is applicable? Okay. I mean, it's the definition of the multi arm bandit problems. Anything else? Something that's not the smart grid, it's not the server, it's not the health care, it's not the driving home, it's not the FFT, please. Many things, what humiliate yourself. So imagine you're looking at a menu at a restaurant, but you can't read the menu. You don't understand the language? No, no, I don't understand the language. I don't speak Greek. Then I just pick something and then maybe it's good, maybe it's bad. This is multi arm bandit problems. Thank you. Because I speak, I read Greek. Okay, that's a nice example. A little bit more engineering. No, I said nothing from the previous examples. Somebody is hungry. Okay. We all understood that, my students, I'm not sure. No, Jan Carlo, it's not in the group like you may find job at face what Facebook does with the online advertisement. Bravo guys. Altogether. I need a 50 to decode here now no one is going to. Okay. He said, let me repeat. For the online people, they always try to target to a person that most probably will give the highest reward back. I don't know what it is, engagement to the advertisement or click and buy shoes. I will make a break afterwards to say the joke that happened during my keynote. It is a multi unbanded problem because it's time. Exactly if I want to take it away from Facebook that someone of you is going to find a job. How we design websites? Give someone the microphone. How do we design websites for website that you would design the website so that it has the most keyword hits for a search return into Google or something like that. Right. The arms will be the I accept it, okay? Or it can be the different arms can be, for example, different designs that I can put on my web page. Like for example, I don't know, I use different colors or letters or something that's fancy to cut somebody's attention. Okay, correct. The design is the action you take and the reward. It's practically how many users visit your website. If tomorrow guys you go in the market and you want to become an assistant professor, make sure you have a very professional website. That will not be the test, it's life advice. Who plays here? Online games. We all understand here what I am applying banded game, He's going to answer we got, okay. In online games, some of them are peer to peer games, games, whatever back end has to decide who should be the host. The action would be someone to be the host. Ward would be the respective latencies of other players. Do we gaming cost make you attract other players? Because the reward, for example, how many other players you attract when you are a cost, when the reward to be how many players you can even support. Because most games have a limit on you gather a lot together. I see now we all understand that there's no way that it will be an online game question at the exam. I have no idea. Nice. Okay, let's continue. Actually, before I continue guys, I need to share the joke. 1 minute break were the ones who had me wait C 341 last semester. Last fall. Nice. Who were the ones that had me in computer networks last spring? Nice guys.
In the epsilon greedy algorithm, how do the values assist the agent in making decisions?Can I repeat? Let me rephrase a little bit. The value, the way it assists in the epsilon gridy algorithm is when you do exploitation in the epsilon greedy from the values that all your actions have, you select the action, as you said, that is characterized by the maximum value. When you do exploration, you select practically any action with random probability. With equal probability. Randomly with equal probability. If in the exam I ask you in the epsilon greedy algorithm, when you perform exploration exploration, the values assist you in any decision. Me, what's your name, kid? That's an easy name. Go ahead. The Q values help you when you explore, because practically you select bravo. Randomly, bravo. Okay, thank you. What does the value of an action represent in a sorry, let me be specific. In epsilon greedy, what the value represents in epsilon greedy isn't like a histogram of the number of actions you took to make things complex. Say that simple. Did you understand what he said? Bravo. That's why I say it's simple. It's not about you. I don't understand what says. It's the history of how many times you took an action. Yeah. Yeah. Histogram, Instagram. Let's give it try to say it's simpler from all of the previous attempts at that action. It's the average of those rewards that you've gotten in the past. Just Just the other, it is the Aber. When you say weighted, what do you mean guys? Remember, in your homework, you have one over alpha. The Q values are weighted, weighted averages. But the weight on, the significance of the weight is now, and things that happened in the past have less weight. Which what you said stems from from the on how many times you have selected this action. Okay? Okay. I don't take wrong the weighted. I wanted the word discounted because that's the word that we are using. So you wait with weight one, your immediate value of that action and with the corresponding discount, one over an alpha, your past. Okay, to get there. I have a question. Who talks, Brian? Isn't that for the non stationary bandit where you're waiting your most recent actions if, if the target is moving. But yeah, I thought in the epsilon greedy all rewards were weighted the same. Well, in the epsilon greedy algorithm, if you see the code in your past rewards, you have one over an alpha there which you can consider it as a discount in the gan bandit algorithm as well, the non stationary one. You also, I think your book has over there the gamma, which also is a discount. In general, all the RL algorithms except the basic Monte Carlo approaches, they put a discount in your history, which is practically that gamma concept. When this gamma concept goes close to one. Do I answer your question or I talk alone? I think I'm still confused I guess in the case where if you have ten actions and each of those actions has like a true average reward and that average reward does not change, why would you wait your most recent rewards over the past rewards? Why would you not just wait them all equally if you see the algorithm epsilon grid, which is that simple banded algorithm that we discussed in the back in the slides, which is practically what you implement in your home Cork. If you see when you update your alpha value, you update that value with whatever you had. Your immediate reward. Oh, what reward you had cumulatively considered so far in order to build that value for the specific action. When I say discount, I'm trying to capture that alpha in front of your history. That's what I'm trying to say. Okay, I think I understand. When we discussed, when we discussed the GT, which practically was the immediate reward plus the expected reward S plus the cumulative reward that you have received from your past. This cumulative reward that you have received from your past, it was multiplied with a gamma. Again, it has the same concept of discount. In general, you are discounting through the discount factor, your history, and you give higher weight, meaning weight equal to one to your present. That's what I'm trying to say. Okay. I think I understand. I might have a follow up question for you tomorrow. Yeah. Okay. Yeah. So what I asked why? Yes. The initial question was how do the Q values assist an agent in making decisions? I asked specifically in the deps long grid. Was that the question that I asked last time? There was another question when I asked, how does an agent and update the Q values, That's what I asked. What is what I asked? Something more basic, James. Don't shake your head. The Q value represents, it represents a linear combination practically of your immediate reward plus the discounted cumulative reward that you have received over the times that you have selected that action. If I ask that in the context of the epsilon, I rephrase the question. In what time is it? Why? Okay, if I asked in the exam, what is the meaning of the return or what the expected return represents in an Rel algorithm, what you're going to reply, it represents a linear combination of the immediate reward, the discounted reward, the discounted cumulative reward that you have received over the times that you have selected the same action, which practically is the GT, which was the RT one plus gamma, your history where gamma was the discount factor. I'll see you on Wednesday. Better for you to watch again the video tonight. Thank you. Bye.
